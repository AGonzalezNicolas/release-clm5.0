<!-- Beg of tools chapter -->
<chapter id="tools">
<title>Using the &clm; tools to create your own input datasets</title>
<para>
There are several tools provided with &clm; that allow you to create your own input 
datasets at resolutions you choose, or to interpolate initial conditions to a different
resolution, or used to compare &clm; history files between different cases. The tools are
all available in the <filename>models/lnd/clm/tools</filename> directory. Most of the tools
are &FORTRAN; stand-alone programs in their own directory, but there is also a suite of
&ncl;
scripts in the <filename>ncl_scripts</filename> directory. Some of the &ncl; scripts are
very specialized and not meant for general use, and we won't document them here. They
still contain documentation in the script itself and the README file in the tools
directory. But, the list of generally important scripts and programs are:
<orderedlist>
<listitem>
<para> <emphasis>cprnc</emphasis> to compare &netcdf; files with a time axis.</para>
</listitem>
<listitem>
<para> <emphasis>interpinic</emphasis> to interpolate initial condition files.</para>
</listitem>
<listitem>
<para> <emphasis>mkgriddata</emphasis> to create grid datasets.</para>
</listitem>
<listitem>
<para> <emphasis>mkdatadomain</emphasis> to create domain files from grid datasets
used by datm or docn.</para>
</listitem>
<listitem>
<para> <emphasis>mksurfdata</emphasis> to create surface datasets from grid datasets.</para>
</listitem>
<listitem>
<para> <emphasis>ncl_scripts/getregional_datasets.pl</emphasis> script to extract a
region or a single-point from global input datasets. See the single-point chapter
for more information on this.</para>
</listitem>
<listitem>
<para> <emphasis>ncl_scripts/npdepregrid.ncl</emphasis> interpolate the Nitrogen
deposition datasets to a new resolution.</para>
</listitem>
<listitem>
<para> <emphasis>ncl_scripts/aerdepregrid.ncl</emphasis> interpolate the Aerosol
deposition datasets to a new resolution.</para>
</listitem>
</orderedlist>
</para>

<para>
In the sections to come we will go into detailed description of how to use each of
these tools in turn. First, however we will discuss the common environment variables
and options that are used by all of the &FORTRAN; tools. Second, we go over the outline
of the entire file creation process for all input files needed by &clm; for a new 
resolution, then we turn to each tool. In the last section we will
discuss how to customize files for particular observational sites.
</para>

<sect1 id="tool_build">
<title>Common environment variables and options used in building the &FORTRAN;
tools</title>
<para>
The &FORTRAN; tools all have similar makefiles, and similar options for building.
All of the Makefiles use GNU Make extensions and thus require that you use GNU make
to use them. They also auto detect the type of platform you are on, using "uname -s"
and set the compiler, compiler flags and such accordingly. There are also environment
variables that can be set to set things that must be customized. All the tools use
&netcdf; and hence require the path to the &netcdf; libraries and include files.
On some platforms (such as Linux) multiple compilers can be used, and hence there
are env variables that can be set to change the &FORTRAN; and/or "C" compilers used.
The tools other than "cprnc" also  allow finer control, by also allowing the user to
add compiler flags they choose for both "C" and &FORTRAN; compilers and set the linker 
and add linker options. Finally the tools other than "cprnc" allow you to turn
optimization on (which is off by default) with the <envar>OPT</envar> flag so that the 
tool will run faster. To get even faster performance, the interpinic, mksurfdata, and
mkgriddata programs allow you to also use the <envar>SMP</envar> to turn on multiple 
shared memory processors.
When <envar>SMP=TRUE</envar> you set the number of threads used by the program with
the <envar>OMP_NUM_THREADS</envar> environment variable.
</para>
<para>
Options used by all: cprnc, interpinic, mkdatadomain, mkgriddata, and mksurfdata
<simplelist>
<member><envar>LIB_NETCDF</envar> -- sets the location of the &netcdf; library.</member>
<member><envar>INC_NETCDF</envar> -- sets the location of the &netcdf; include files.</member>
<member><envar>USER_FC</envar> -- sets the name of the &FORTRAN; compiler.</member>
</simplelist>
Options used by: interpinic, mkdatadomain, mkgriddata, and mksurfdata
<simplelist>
<member><envar>MOD_NETCDF</envar> -- sets the location of the &netcdf; &FORTRAN; module.</member>
<member><envar>USER_LINKER</envar> -- sets the name of the linker to use.</member>
<member><envar>USER_CPPDEFS</envar> -- adds any CPP defines to use.</member>
<member><envar>USER_CFLAGS</envar> -- add any "C" compiler flags to use.</member>
<member><envar>USER_FFLAGS</envar> -- add any &FORTRAN; compiler flags to use.</member>
<member><envar>USER_LDFLAGS</envar> -- add any linker flags to use.</member>
<member><envar>USER_CC</envar> -- sets the name of the "C" compiler to use.</member>
<member><envar>OPT</envar> -- set to TRUE to compile the code optimized (TRUE or FALSE)</member>
</simplelist>
Options used by: interpinic, mkgriddata, and mksurfdata:
<simplelist>
<member><envar>SMP</envar> -- set to TRUE to turn on shared memory parallelism (i.e.
OpenMP) (TRUE or FALSE)</member>
<member><filename>Filepath</filename> -- list of directories to build source code from.</member>
<member><filename>Srcfiles</filename> -- list of source code filenames to build executable from.</member>
</simplelist>
Options used only by cprnc:
<simplelist>
<member><envar>EXEDIR</envar> -- sets the location where the executable will be built.</member>
<member><envar>VPATH</envar> -- colon delimited path list to find the source files.</member>
</simplelist>
More details on each environment variable.
<variablelist>
<varlistentry>
<term><envar>LIB_NETCDF</envar></term><listitem> 
<para>
This variable sets the path to the &netcdf; library file 
(<filename>libnetcdf.a</filename>). If not 
set it defaults to <filename>/usr/local/lib</filename>. In order to use the tools
you need to build the &netcdf; library and be able to link to it. In order to build
the model with a particular compiler you may have to compile the &netcdf; library with
the same compiler (or at least a compatible one).
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>INC_NETCDF</envar></term><listitem> 
<para>
This variable sets the path to the &netcdf; include directory (in order to find
the include file <filename>netcdf.inc</filename>).
if not set it defaults to <filename>/usr/local/include</filename>.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>MOD_NETCDF</envar></term><listitem> 
<para>
This variable sets the path to the &netcdf; module directory (in order to find
the &netcdf; &FORTRAN90; module file when &netcdf; is used with a &FORTRAN90;
<command>use statement</command>. When not set it defaults to the 
<envar>LIB_NETCDF</envar> value.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_FC</envar></term><listitem> 
<para>
This variable sets the command name to the &FORTRAN90; compiler to use when
compiling the tool. The default compiler to use depends on the platform. And
for example, on the AIX platform this variable is NOT used
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_LINKER</envar></term><listitem> 
<para>
This variable sets the command name to the linker to use when linking the object
files from the compiler together to build the executable. By default this is set to
the value of the &FORTRAN90; compiler used to compile the source code.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_CPPDEFS</envar></term><listitem> 
<para>
This variable adds additional optional values to define for the C preprocessor.
Normally, there is no reason to do this as there are very few CPP tokens in the CLM
tools. However, if you modify the tools there may be a reason to define new CPP
tokens.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_CC</envar></term><listitem> 
<para>
This variable sets the command name to the "C" compiler to use when
compiling the tool. The default compiler to use depends on the platform. And
for example, on the AIX platform this variable is NOT used
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_CFLAGS</envar></term><listitem> 
<para>
This variable adds additional compiler options for the "C" compiler to use
when compiling the tool. By default the compiler options are picked according
to the platform and compiler that will be used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_FFLAGS</envar></term><listitem> 
<para>
This variable adds additional compiler options for the &FORTRAN90; compiler to use
when compiling the tool. By default the compiler options are picked according
to the platform and compiler that will be used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_LDFLAGS</envar></term><listitem> 
<para>
This variable adds additional options to the linker that will be used when linking
the object files into the executable. By default the linker options are picked according
to the platform and compiler that is used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>SMP</envar></term><listitem> 
<para>
This variable flags if shared memory parallelism (using OpenMP) should be used when 
compiling the tool.  It can be set to either <literal>TRUE</literal> or 
<literal>FALSE</literal>, by default it is set to <literal>FALSE</literal>, so 
shared memory parallelism is NOT used. When set to <literal>TRUE</literal> you can
set the number of threads by using the <envar>OMP_NUM_THREADS</envar> environment
variable. Normally, the most you would set this to would be to the number of on-node
CPU processors. Turning this on should make the tool run much faster.
</para>
<caution>
<para>
Note, that depending on the compiler answers may be different when <envar>SMP</envar>
is activated.
</para>
</caution>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>OMP</envar></term><listitem> 
<para>
This variable flags if compiler optimization should be used when 
compiling the tool.  It can be set to either <literal>TRUE</literal> or 
<literal>FALSE</literal>, by default it is set to <literal>FALSE</literal>, so 
compiler optimization is NOT used. Turning this on should make the tool run much faster.
</para>
<caution>
<para>
Note, you should expect that answers will be different when <envar>OMP</envar>
is activated.
</para>
</caution>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>Filepath</filename></term><listitem> 
<para>
"cprnc" unlike the other &clm; tools doesn't need any outside code to operate. The
other tools are use some code outside their directory that is in the &ccsm;
distribution (either csm_share code or &clm; source code). They all have a 
<filename>Filepath</filename> file to set the location of the source code
directories that will be used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>Srcfiles</filename></term><listitem> 
<para>
The <filename>Srcfiles</filename> lists the filenames of the source code to use
when building the tool.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>EXEDIR</filename></term><listitem> 
<para>
The "cprnc" tool uses this variable to set the location of where the executable
will be built. The default is the current directory.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>VPATH</filename></term><listitem> 
<para>
The "cprnc" tool uses this variable to set the colon delimited pathnames of where
the source code exists.  The default is the current directory.
</para>
</listitem>
</varlistentry>

</variablelist>
</para>

</sect1>

<sect1 id="file_creation_process">
<title>The File Creation Process</title>

<para>
When just creating a replacement file for an existing one, the relevant tool should
be used directly to create the file. When you are creating a set of files for a new
resolution there are some dependencies between the tools that you need to keep in mind
when creating them. The main dependency is that the <command>mkgriddata</command> MUST
be done first as the grid dataset is then input into the other tools. Also look at
<xref linkend="table_required_files"></xref>.
</para>

<procedure>
<title>Creating a complete set of files for input to &clm;</title>
<step>
<title>Create grid and fraction datasets</title>
<para>
First use <command>mkgriddata</command> to create grid and fraction datasets.
See <xref linkend="mkgriddata"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Create domain dataset (if NOT already done)</title>
<para>
Next use <command>mkdatadomain</command> to create a domain file for use by
datm from the grid and fraction datasets just created. This is required, unless
a domain file already created was input into <command>mkgriddata</command> on
the previous step.
See <xref linkend="mkdatadomain"></xref> for more information on this.
</para>
</step>

<step>
<title>Create surface datasets</title>
<para>
Next use <command>mksurfdata</command> to create a surface dataset, using the grid
dataset as input.
See <xref linkend="mksurfdata"></xref> for more information on this.
</para>
</step>

<step>
<title>Create aerosol deposition datasets</title>
<para>
Next use <command>aerdepregrid.ncl</command> to regrid aerosol deposition datasets to 
your new resolution using the grid dataset as input.
See <xref linkend="aerdepregrid"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Create Nitrogen deposition datasets (if running &clmcn;)</title>
<para>
Next use <command>ndepregrid.ncl</command> to regrid Nitrogen deposition datasets to 
your new resolution using the grid dataset as input. This is required if you are using
&clmcn; and otherwise is NOT.
See <xref linkend="ndepregrid"></xref> for more information on this.
</para>
</step>

<step>
<title>Create some sort of initial condition dataset</title>

<para>
You then need to do one of the following three options to have an initial dataset
to start from.
</para>

<stepalternatives>

<step performance="optional">
<title>Use spinup-procedures to create initial condition datasets</title>
<para>
The first option is to do the spinup procedures from arbitrary initial conditions
to get good initial datasets. This is the most robust method to use.
See <xref linkend="CLMSP_SPINUP"></xref>, <xref linkend="CN_SPINUP"></xref>, or 
<xref linkend="CNDV_SPINUP"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Use <command>interpinic</command> to create interpolate existing initial
condition datasets</title>
<para>
The next option is to interpolate from spunup datasets at a different resolution, using
<command>interpinic</command>.
See <xref linkend="interpinic"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Start up from arbitrary initial conditions</title>
<para>
The last alternative is to run from arbitrary initial conditions without using any
spun-up datasets. This is inappropriate when using &clmcn; (bgc=cn or cndv) as it 
takes a long time to spinup Carbon pools. 
<warning>
<para>
This is NOT recommended as many fields in &clm; take a long time to equilibrate.
</para>
</warning>
</para>
</step>

</stepalternatives>

</step>

<step performance="optional">
<title>Enter the new datasets into the &buildnml; XML database</title>
<para>
The last optional thing to do is to enter the new datasets into the &buildnml;
XML database. See <xref linkend="adding_files"></xref> for more information on
doing this. This is optional because the user may enter these files into their
namelists manually. The advantage of entering them into the database is so that
they automatically come up when you create new cases.
</para>
</step>

</procedure>

</sect1>

<sect1 id="cprnc">
<title>Using the cprnc tool to compare two history files</title>
<para>
"cprnc" is a tool shared by both <acronym>CAM</acronym> and &clm; to compare two
&netcdf; history files.
It differences every field that has a time-axis that is also shared on both files,
and reports a summary of the difference. The summary includes the three largest 
differences, as well as the root mean square (RMS) difference. It also gives some
summary information on the field as well. You have to enter at least one file, and up to
two files. With one file it gives you summary information on the file, and with two it
gives you information on the differences between the two. At the end it will give you a
summary of the fields compared and how many fields were different and how many were
identical.
</para>
<para>
Options:
<simplelist>
<member>-m = do NOT align time-stamps before comparing</member>
<member>-v = verbose output</member>
<member>-ipr</member>
<member>-jpr</member>
<member>-kpr</member>
</simplelist>
See the "cprnc"
<ulink url="../../tools/cprnc/README">README</ulink> file for more details.
</para>
</sect1>

<sect1 id="interpinic">
<title>Using interpinic to interpolate initial conditions to different
resolutions</title>
<para>
"interpinic" is used to interpolate initial conditions from one resolution to another.
In order to do the interpolation you must first run &clm; to create a restart file to
use as the "template" to interpolate into. Running from arbitrary initial conditions
(i.e. finidat = ' ') for a single time-step is sufficient to do this. Make sure the
model produces a restart file. You also need to make sure that you setup the same
configuration that you want to run the model with, when you create the template file.
</para>
<para>
Command line options to <command>interpinic</command>:
<simplelist>
<member>-i = Input filename to interpolate from</member>
<member>-o = Output interpolated file, and starting template file</member>
</simplelist>
</para>
<para>
There is a sample template file in the <filename>models/lnd/clm/tools/interpinic</filename>
directory and can be used to run interpolate to.
However, this file was created with an older version of &clm; and hence
we actually recommend that you would do a short run with &clm; to create a template file
to use.
</para>
<para>

<example id="example_createtemplate">
<title>Example running &clm; to create a template file to interpolate to</title>
<screen width="99">
> cd scripts
> create_newcase -case cr_f10_TmpltI1850CN -res f10_f10 -compset I1850CN -mach bluefire
> cd cr_f10_TmpltI1850CN
# Set starting date to end of year
> xmlchange -file env_conf.xml -id RUN_STARTDATE -val 1948-12-31
# Set year align to starting year
> xmlchange -file env_conf.xml -id DATM_CLMNCEP_YR_ALIGN -val 1948
# Set to run a cold start
> xmlchange -file env_conf.xml -id CLM_FORCE_COLDSTART -val on
# Set to run only a single day, so a restart file will be created on Jan/1/1949
> xmlchange -file env_run.xml -id STOP_N -val 1
# Then configure, build and run as normal
> configure -case
> cr_f10_TmpltI1850CN.bluefire.build
> bsub < cr_f10_TmpltI1850CN.bluefire.run
# And copy the resulting restart file to your interpinic directory
> cd ../models/lnd/clm/tools/interpinic
> cp /ptmp/$LOGIN/cr_f10_TmpltI1850CN/run/cr_f10_TmpltI1850CN.clm2.r.1949-01-01-00000.nc .
</screen>
</example>
</para>
<para>
In the next example we build <command>interpinic</command> optimized with shared
memory on for 64 threads so that it runs as fast as possible, to interpolate one of
the standard 1-degree datasets to the above 10x15 template file that we created.
</para>
<example id="example_interpinic">
<title>Example building and running interpinic to interpolate a 1-degree dataset to
10x15</title>
<screen>
> cd models/lnd/clm/tools/interpinic
> gmake OPT=TRUE SMP=TRUE
> env OMP_NUM_THREADS=64 interpinic -o cr_f10_TmpltI1850CN.clm2.r.1949-01-01-00000.nc /
-i /fs/cgd/csm/inputdata/ccsm4_init/b40.1850.track1.1deg.006/0863-01-01/b40.1850.track1.1deg.006.clm2.r.0863-01-01-00000.nc
</screen>
<para>
<tip>
<para>
Running <command>interpinic</command> at high resolution can take a long time, so we
recommend that you always build it optimized and with shared memory processing on, to
cut down the run time as much as possible.
</para>
</tip>
<warning>
<para>>
<command>interpinic</command> does NOT work for CNDV (bgc=cndv).
</para>
</warning>
</para>
</example>

<para>
In <xref linkend="runinit_ibm.csh"></xref> we give a simplier way to run
<command>interpinic</command> for several standard resolutions at once, with a script
to loop over several resolutions. This is useful for &clm; developers who need to
create many <filename>finidat</filename> files at once.
</para>

</sect1>

<sect1 id="mkgriddata">
<title>Using mkgriddata to create grid datasets</title>
<para>
"mkgriddata" is used to create grid, fraction and topography datasets to run &clm;
at a new resolution. It is typically the first step in creating datasets needed
to run &clm; at a new resolution (followed by mksurfdata, and then the interpolation
programs, aerdepregrid, and ndepregrid when running with CN).
</para>

<sect2 id="mkgriddata_nml">
<title>mkgriddata namelist</title>
<para>
<command>mkgriddata</command> is controlled by a namelist. There are ten different
namelist items, and you need to use enough of them so that files will be output.
The different types of input datasets contain different input data types that
correspond to the three different types of output files: grid, fraction, and topography.
Output files for each of these will only be output if there is input data that 
correspond to these. If you only have input data for grid locations -- you will only
get an output grid file. If you have both grid and fraction data you will get grid and
fraction data files. If you also have topography data you will also get topo files.
</para>
<para>
Namelist options to mkgriddata include:
<simplelist>
<member><varname>mksrf_fnavyoro</varname> -- Navy orography file to use for land fraction
and surface heights.</member>
<member><varname>mksrf_frawtopo</varname> -- Raw topography file with just surface
heights.</member>
<member><varname>mksrf_fcamfile</varname> -- <acronym>CAM</acronym> initial conditions file with
land-fractions and topography</member>
<member><varname>mksrf_fclmgrid</varname> -- &clm; grid file</member>
<member><varname>mksrf_fccsmdom</varname> -- &ccsm; domain file</member>
<member><varname>mksrf_fcamtopo</varname> -- <acronym>CAM</acronym> topography file</member>
<member><varname>mksrf_lsmlon</varname> -- number of longitude for regional grid</member>
<member><varname>mksrf_lsmlat</varname>number of latitudes for regional grid</member>
<member><varname>mksrf_edgen</varname> -- Northern edge for regional grid</member>
<member><varname>mksrf_edgee</varname> -- Southern edge for regional grid</member>
<member><varname>mksrf_edges</varname> -- Eastern edge for regional grid</member>
<member><varname>mksrf_edgew</varname> -- Western edge for regional grid</member>
</simplelist>
</para>
<para>
You need to enter one of the following four options:
<screen>
   mksrf_fnavyoro    - high resolution topo dataset (topo data)
   mksrf_lsmlon      - number of longitudes
   mksrf_lsmlat      - number of latitudes
   mksrf_edgen       - northern edge of grid (degrees)
   mksrf_edgee       - eastern  edge of grid (degrees)
   mksrf_edges       - southern edge of grid (degrees)
   mksrf_edgew       - western  edge of grid (degrees)
</screen>
or
<screen>
   mksrf_fcamfile    - cam topo file (grid and possibly fraction data)
</screen>
or 
<screen>
   mksrf_fccsmdom    - ccsm domain file (both grid, and fraction data)
</screen>
or
<screen>
   mksrf_fclmgrid    - clm grid or surface dataset file (grid data)
</screen>
Note, you can provide more than one of the needed datasets, and the output
data will be determined by the datasets according to an order of precedence.
The order of precedence for data is as follows:
<orderedlist>
<listitem><para><filename>mksrf_fcamfile</filename></para></listitem>
<listitem><para><filename>mksrf_fclmgrid</filename></para></listitem>
<listitem><para><filename>mksrf_fnavyoro</filename></para></listitem>
<listitem><para><filename>mksrf_fccsmdom</filename></para></listitem>
</orderedlist>
Grid data then will be established by the file with the highest precedence. 
&ccsm; domain files sometimes have latitudes and longitudes that are "off" from
the standard by a small amount. By establishing an order of precedence you can ensure
that grid locations exactly match a given standard file, even if the values in the domain
file are off from that.
</para>

<para>
There are three different major modes for using "mkgriddata" to create grid files
for &clm;:
<simplelist>
<member>Convert &ccsm; domain files to &clm; grid files</member>
<member>Create single point or regional area grid files</member>
<member>Convert <acronym>CAM</acronym> files to &clm; grid files</member>
</simplelist>
</para>
</sect2>

<sect2 id="mkgridata_from_domainfile">
<title>Convert &ccsm; domain files to &clm; grid files</title>
<para>
&ccsm; domain files such as used for datm, include all the information
needed to create &clm; grid and fraction files.
<example>
<title>Example namelist to convert &ccsm; 4x5 domain files to &clm; grid files</title>
<screen>
&amp;clmexp
 mksrf_fccsmdom=
'/fs/cgd/csm/inputdata/lnd/dlnd7/domain.lnd.4x5_gx3v5.060404.nc'
 mksrf_fclmgrid=
'/fs/cgd/csm/inputdata/lnd/clm2/griddata/griddata_4x5_060404.nc'
/
</screen>
</example>
<tip>
<para>
Notice that in the above example, a clm grid file is included as well, even though
it's not required. The reason for this is to ensure that the latitude and longitudes
on the output files exactly match a standard grid file.
</para>
</tip>
</para>
</sect2>

<sect2 id="mkgridata_regional">
<title>Create single point or regional area grid files</title>
<para>
The process to create single-point or regional area &clm; grid files is the same.
You enter the number of latitudes and longitudes you want on your output file and
the extent of the grid: North, East, South and West. You also tell "mkgriddata" that
you are entering a "regional" grid and you also enter the standard Navy orography
dataset (or your own orography file if desired). For a single point you simply
enter "1" for the number of latitudes and longitudes, but you still enter the
grid extent (of the single grid cell). Here is a sample regional namelist to create
a 5x5 regional grid over the Amazon:
<example>
<title>Example namelist for mkgriddata to create regional grid over Amazon</title>
<screen>
&amp;clmexp
 mksrf_fnavyoro=
"/fs/cgd/csm/inputdata/lnd/clm2/rawdata/mksrf_navyoro_20min.c010129.nc"
 mksrf_lsmlon = 5
 mksrf_lsmlat = 5
 mksrf_edgee = 303.75
 mksrf_edgew = 286.25
 mksrf_edges = -15.
 mksrf_edgen = -4.
/
</screen>
</example>
</para>
<important>
<para>
You should enter longitudes with values from 0 to 360 East.
</para>
</important>
</sect2>

<sect2 id="mkgridata_from_camfile">
<title>Convert <acronym>CAM</acronym> files to &clm; grid files</title>
<para>
Older <acronym>CAM</acronym> initial files included all the information needed to create &clm;
grid files. Newer <acronym>CAM</acronym> files no longer include land fraction data. Hence you
can use <acronym>CAM</acronym> files to give you the grid coordinates, but you need other data
to give you the land-mask and topography. Since, <acronym>CAM</acronym> files no longer
contain the needed information, this option is now deprecated. In most cases you should
use one of the other two options.
</para>
</sect2>


</sect1>

<sect1 id="mkdatadomain">
<title>Using mkdatadomain to create domain datasets for datm or docn from &clm; grid datasets</title>
<para>
"mkdatadomain" is used to convert &clm; grid and fraction datasets into domain datasets
that can be used by either the "datm" or "docn" models. Most often &clm; users will want
to convert the grid datasets they just created using <command>mkgriddata</command> into
domain datasets to be used by datm for an "I" case. <command>mkdatadomain</command> is
controlled by a namelist, and has a very straight forward operation with only four
namelist items all of which are required. You specify which output mode you want "datm"
or "docn", and then set the input &clm; grid and frac datasets, and the output domain file.
</para>
<example>
<title>Example namelist to create domain file from &clm; frac and grid data files</title>
<screen>
&amp;domain_nl
 dtype = "datm"
 f_fracdata = 
'/fs/cgd/csm/inputdata/lnd/clm2/griddata/fracdata_4x5_USGS_070110.nc'
 f_griddata = 
'/fs/cgd/csm/inputdata/lnd/clm2/griddata/griddata_4x5_060404.nc'
 f_domain   = 
'domain.lnd.fv4x5_USGS.090117.nc'
/
</screen>
</example>
</sect1>

<sect1 id="mksurfdata">
<title>Using mksurfdata to create surface datasets from grid datasets</title>
<para>
"mksurfdata" is used to create surface-datasets from grid datasets and raw datafiles
at half-degree resolution to produce files that describe the surface characteristics
needed by &clm; (fraction of grid cell covered by different land-unit types, and fraction
for different vegetation types, as well as things like soil color, and soil texture,
etc.).
</para>
</sect1>

<sect1 id="ncl_scripts">
<title>Using &ncl; scripts ndepregrid.ncl and aerdepregrid.ncl to interpolate aerosol deposition datasets</title>
<para>
Unlike the other tools, these are NCAR Command Language (&ncl;) scripts
and you will need to get a copy of &ncl; in order to use them. You also won't have to
build an executable in order to use them, hence no Makefile is provided. &ncl; is provided
for free download as either binaries or source code from:
<ulink url="http://www.ncl.ucar.edu/">http://www.ncl.ucar.edu/</ulink>. The &ncl;
web-site also contains documentation on &ncl; and it's use.
</para>
<para>
Both the <filename>ndepregrid.ncl</filename> and <filename>aerdepregrid.ncl</filename>
scripts have similar interfaces and you customize the output resolution and
characteristics based on the settings of environment variables that you set (if you
don't set any of the variables, the script has defaults that it will use).
The list of environment variables that can be set are:
<simplelist>
<member><envar>RES</envar> -- output resolution name</member>
<member><envar>RCP</envar> -- representative concentration pathway for future scenarios
(example 2.6, 4.5, 6, or 8.5)</member>
<member><envar>SIM_YR</envar> -- simulation year (example 1850 or 2000)</member>
<member><envar>SIM_YR_RNG</envar> -- simulation year range (example 1850-2000 or
1850-2100)</member>
<member><envar>GRDFIL</envar> -- full pathname of grid file to use 
(in place of getting the default grid file based on the <envar>RES</envar> value)</member>
<member><envar>CSMDATA</envar> -- &ccsm; inputdata directory</member>
<member><envar>CLM_ROOT</envar> -- root directory for clm (models/lnd/clm directory)</member>
</simplelist>
</para>
<important>
<para>
You <emphasis>MUST</emphasis> provide either <envar>RES</envar> or both
<envar>GRDFIL</envar> <emphasis>AND</emphasis> <envar>RES</envar>. If you
just give <envar>RES</envar> the default namelist database in
<filename>models/lnd/clm/bld</filename> will be used to find the default grid
file based on the resolution name <envar>RES</envar>. If you provide
<envar>GRDFIL</envar> the input pathname of the gridfile provided will be used,
and the output filename will include <envar>RES</envar> as part of it's name
to designate it as an output file at that resolution.
</para>
</important>
<para>
Both scripts assume that you will be interpolating from a native resolution of 1.9x2.5
and using the default files found in the namelist database to interpolate from. If you
want to interpolate from another resolution or use other files, you would need to edit
the scripts to do so. Both scripts also use a bilinear interpolation to do the
regridding. The environment variables: <envar>RCP</envar>, <envar>SIM_YR</envar>,
and <envar>SIM_YR_RNG</envar> will be used to query the namelist database to 
determine which native dataset to interpolate from. If you don't provide valid
values for these variables, it won't be able to find a dataset to interpolate from.
You can use the build-namelist script to query what the valid values for these can
be. Likewise, when you use <envar>RES</envar> to determine the grid file to interpolate
to, it needs to be a valid value from the namelist database.
</para>
<para>
The scripts can be used to interpolate from (and create output) constant or
transient datasets.
Constant datasets specify the <envar>SIM_YR</envar> and set <envar>SIM_YR_RNG</envar>
to <varname>constant</varname> (which is also the default). Transient datasets need
to specify both <envar>SIM_YR</envar> and <envar>SIM_YR_RNG</envar>, where 
<envar>SIM_YR</envar> is set to the first year in the interval (typically 1850).
</para>
<para>
The default for <envar>CSMDATA</envar> works for NCAR computers, but will need to
be set to the top level directory location of your &ccsm; input data on other computers.
If you set this as a default for your shell when you login (for example with your
<filename>$HOME/.cshrc</filename> if you use csh) you won't have to set it each time
you run the script. <envar>CLM_ROOT</envar> will default to the proper location
when you run it in the <filename>models/lnd/clm/tools/ncl_script</filename>
directory. It is only useful if you want to run the script out of a different 
directory.
</para>
<sect2 id="ndepregrid">
<title>Using ndepregrid.ncl to interpolate Nitrogen deposition datasets</title>
<para>
"ndepregrid.ncl" interpolates the Nitrogen deposition datasets from one resolution
to another.  It can be used to interpolate either constant datasets
(<varname>fndepdat</varname> files) or transient datasets (<varname>fndepdyn</varname> files).
</para>
<para>
For example, to interpolate to an output resolution of 0.9x1.25, for a constant
simulation-year of 1850, you would do the following:
<screen>
> env RES=0.9x1.25 SIM_YR=1850 ncl ndepregrid.ncl
</screen>
</para>
</sect2>

<sect2 id="aerdepregrid">
<title>Using aerdepregrid.ncl to interpolate Aerosol deposition datasets</title>
<para>
"aerdepregrid.ncl" interpolates the Aerosol deposition datasets from one resolution.
It can be used to interpolate either constant datasets (for example:
aerosoldep_monthly_2000_0.9x1.25_c090828.nc) or transient datasets (for example:
aerosoldep_monthly_1849-2006_0.9x1.25_c090830.nc). The aerosol datasets unlike the
Nitrogen deposition datasets, are named the same <varname>faerdep</varname> whether
they contain one time-sample or a time-series of data. But, in any case, this script
can be used to interpolate either kind of file.
</para>
<para>
For example, to interpolate to an output resolution of 4x5, for a transient
simulation-year range of 1850 to 2100 and the rcp of 8.5, you would do the following:
<screen>
> env RES=4x5 SIM_YR=1850 SIM_YR_RNG=1850-2100 RCP=8.5 ncl ndepregrid.ncl
</screen>
</para>
</sect2>
</sect1>

<sect1 id="customizing_files">
<title>How to Customize Datasets for particular Observational Sites</title>
<para>
There are two ways to customize datasets for a particular observational site. The first
is to customize the input to the tools that create the dataset, and the second is to
over-write the default data after you've created a given dataset. Depending on the tool
it might be easier to do it one way or the other. In <xref
linkend="table_required_files"></xref> we list the files that are most likely to be
customized and the way they might be customized. Of those files, the ones you are most
likely to customize are: fatmlndfrc, fsurdat, faerdep, and fndepdep.
</para>
</sect1>

<sect1 id="tools_conclude">
<title>Conclusion of tools description</title>
<para>
We've given a description of how to use the different tools with &clm; to create
customized datasets. In the next chapter we will talk about how to make these
files available for build-namelist so that you can easily create simulations
that include them. In the chapter on single-point and regional datasets we also
give an alternative way to enter new datasets without having to edit files.
</para>
</sect1>

</chapter>
<!-- End of tools chapter -->
