Known Bugs in CLM3.6    		                          Jan/17/2009

====================================================================================
Bug Number: 789
single point simulations in ccsm_seq mode run 2.5X slower than in offline mode

I ran a 9 year single point simulation in offline mode and one in ccsm_seq
mode.  Jobs were both submitted to bluevista regular queue.  The ccsm_seq
simulation took about 2.5X longer than the offline one.  It appears most of the
extra time is spent in the datm.  Here are some timing results:

Offline:
runtotal   1201s

Ccsm_seq:
DRIVER_RUN_LOOP   3063s
  DRIVER_LND_RUN  1184s
  DRIVER_ATM_RUN  1817s

In the log file more timing results:

datm_phys_dataIn - everything   1614s
datm_phys_dataIn - read data    1584s

So it seems like the datm routines that read in data are taking up more time
than the model itself.

====================================================================================
Bug Number: 887
MPI_INTEGER8 causes trouble for serial codes using MCT/mpi-serial

I ran into a bunch of trouble with the latest csm_share share3_090112 includes
the use of MPI_INTEGER8. This caused me a bunch of trouble when I try to
compile it in serial mode (on lightning or bangkok for example).

I'm also a little bit nervous about this token as it appears that MPI_INTEGER8
isn't in the MPI standard. The standard does require something like it if the
underlying compiler has something like INTEGER*8 -- but it doesn't specify what
it should be.

So we could get around this by adding it to mpif.h in mct/mpi-serial -- but I'm
just wondering if it's available as MPI_INTEGER8 for all of the versions of MPI
that we use?

This also effects the clm offline tools as they use csm_share stuff. I was able
to get around it by adding an #ifdef NO_MPI_INT8 for the tools, but the driver
uses this capability so it won't compile without the new stuff.

====================================================================================
Bug Number: 717
Problem with lt_archiving for too-many files

Keith has been running into a bunch of problems with the lt_archiving when
using CLM. He can get 30 years in a 6 hour wall-clock cycle -- which means
overa thousand files -- where lt_archive.sh pukes.

Making it robust -- regardless of the number of files would be one first step.
The next problem he has is that running over just 10 years -- the job resubmits
itself before the lt_archive script is done. Then the scripts conflict and he
ends up having to run the archiving by hand. It would also help if it were
easier to submit the lt_archive.sh script as well.

====================================================================================
Bug Number: 885
Inconsistent datasets for clm and datm domain files for some resolutions

Resolutions 64x128_gx3v5, 64x128_gx1v5 and 48x96_gx3v5 all fail because of inconsistencies
in the inputdata datasets.

====================================================================================
Bug Number: 877
Restart problem for CN in clm36sci27_clm3_6_14

Restart tests are failing for CN.

====================================================================================
Bug Number: 452
Problem with support of number of soil-colors NOT equal to 8 or 20

The mksurfdata tools file mksoicol.F90 sets nsoicol to the max value found in
the input soilcolor file:
  nsoicol = maxval(soil_color_i)  

However, the code will fail if  nsoicol does not equal 20 or 8 (which it might
in paleo cases).  perhaps the code should be extended to handle a case where
nsoicol is not 20 or 8.

====================================================================================
Bug Number: 361
Problem with exact restarts on SGI

Restarts do NOT work correctly and do NOT give exact answers as a simulation that runs continusly.

These are the list of tests from test/system/test_driver.sh that fail, because of
this. TER tests are exact restart tests and TBR tests are branch tests -- testing
that answers are exact when namelist items remain unchanged.

002 er111 TER.sh 4p_vodsr_dh t31 10+38 ............................FAIL! rc= 11
003 br111 TBR.sh 4p_vodsr_dh t31 24+24 ............................FAIL! rc= 11
005 sm116 TSM.sh 4p_vodsr_o t31 48 ................................FAIL! rc= 4
007 er121 TER.sh 17p_vodsr_dh t31 10+38 ...........................FAIL! rc= 11
008 br121 TBR.sh 17p_vodsr_dh t31 24+24 ...........................FAIL! rc= 11
012 er211 TER.sh 17p_cnn_dh t31_cnall 10+38 .......................FAIL! rc= 11
013 br211 TBR.sh 17p_cnn_dh t31_cnall 24+24 .......................FAIL! rc= 11
016 er311 TER.sh 4p_casa_dh t31_casa 10+38 ........................FAIL! rc= 11
017 br311 TBR.sh 4p_casa_dh t31_casa 24+24 ........................FAIL! rc= 11
020 er411 TER.sh 10p_dgvm_dh t31_dgvm 10+38 .......................FAIL! rc= 11
021 br411 TBR.sh 10p_dgvm_dh t31_dgvm 24+24 .......................FAIL! rc= 11

====================================================================================
Bug number: 447
Problem running on Cray-X1

Offline CLM will NOT run on Cray-X1 because of calls to shr_sys_flush(6) without
unit 6 being explicitly opened. 

====================================================================================
Bug number: 652
Output different for different number of threads PGF90 

PGI Version 6.1.6, and NetCDF Version 3.6.2. Works on other platforms/compilers and
also works with PGI-7.0-7.

====================================================================================
Bug number: 546
clm-offline-tools: interpinic does NOT work for DGVM

interpinic does NOT work for DGVM. OK as CNDV will replace DGVM.

====================================================================================
